# ===================================
# DOCKER COMPOSE CONFIGURATION - DEVELOPMENT
# ===================================
# Environment variables for docker-compose.dev.yaml
# This is a development configuration with Watchtower enabled
#
# Usage:
#   cp .env.dev .env  # If you want to use dev settings
#   docker compose -f docker-compose.dev.yaml up -d

# ===================================
# GLOBAL SETTINGS
# ===================================
# Restart policy for all services (unless-stopped, always, no, on-failure)
RESTART_POLICY=always

# ===================================
# OLLAMA CONFIGURATION
# ===================================
# Ollama service settings
OLLAMA_IMAGE=ollama/ollama:latest
OLLAMA_CONTAINER_NAME=aistack-ollama
OLLAMA_PORT=11434
OLLAMA_HOST=0.0.0.0

# CORS (Cross-Origin Resource Sharing) allowed origins
# Controls which web domains can access the Ollama API
# "*" = Allow all origins (default, convenient for development)
# For production, specify exact origins for security:
# OLLAMA_ORIGINS=http://localhost:3000,https://yourdomain.com
OLLAMA_ORIGINS=*

# VRAM Management (Critical for 16GB GPU with Ollama + ComfyUI)
# Ollama: Allocate max 10GB VRAM (leaves ~6GB for ComfyUI)
OLLAMA_MAX_VRAM=10737418240
# Limit to 2 loaded models simultaneously to prevent memory exhaustion
OLLAMA_MAX_LOADED_MODELS=2
# Allow 2 parallel requests (balance between throughput and memory usage)
OLLAMA_NUM_PARALLEL=2

# ===================================
# OPEN-WEBUI CONFIGURATION
# ===================================
# Open-WebUI service settings
OPENWEBUI_IMAGE=ghcr.io/open-webui/open-webui:main
OPENWEBUI_CONTAINER_NAME=aistack-open-webui
OPENWEBUI_PORT=3000

# Open-WebUI application settings
OLLAMA_BASE_URL=http://aistack-ollama:11434
WEBUI_SECRET_KEY=dev-secret-key-change-for-production
WEBUI_AUTH=true
WEBUI_NAME="Open WebUI (Dev)"

# ===================================
# COMFYUI CONFIGURATION
# ===================================
# ComfyUI service settings
COMFYUI_IMAGE=COMFYUI_IMAGE=ghcr.io/saladtechnologies/comfyui-api:comfy0.3.67-api1.13.3-torch2.8.0-cuda12.8-runtime
COMFYUI_CONTAINER_NAME=aistack-comfyui
COMFYUI_PORT=8188
COMFYUI_INTERNAL_PORT=8188

# VRAM Management for ComfyUI
# --normalvram: Balanced memory mode (recommended for shared GPU)
# Other options:
#   --lowvram: Use system RAM when VRAM is full (slower but safer)
#   --novram: Keep models in system RAM (very slow, maximum compatibility)
#   --highvram: Keep everything in VRAM (fast but uses more memory)
COMFYUI_CLI_ARGS=--normalvram

# ===================================
# WATCHTOWER CONFIGURATION (ENABLED FOR DEVELOPMENT)
# ===================================
# üîÑ Watchtower automatically updates containers when new images are available
# ‚ö†Ô∏è  This is enabled for development convenience
# üîí For production, disable this and use ./update-stack.sh instead

# Watchtower service settings
WATCHTOWER_IMAGE=containrrr/watchtower:latest
WATCHTOWER_CONTAINER_NAME=aistack-watchtower

# Watchtower behavior settings
WATCHTOWER_CLEANUP=true
WATCHTOWER_INCLUDE_STOPPED=false

# Update schedule (cron format: minute hour day month weekday)
# Current: Every day at 3 AM
# Examples:
#   0 0 * * * = Every day at midnight
#   0 */6 * * * = Every 6 hours
#   0 0 * * 0 = Every Sunday at midnight
WATCHTOWER_SCHEDULE=0 0 3 * * *

# Enable debug logging (helpful for development)
WATCHTOWER_DEBUG=false

# Watchtower notifications (optional)
# Leave empty to disable notifications
WATCHTOWER_NOTIFICATIONS=
WATCHTOWER_NOTIFICATION_URL=

# Example notification URLs:
# - Slack: https://hooks.slack.com/services/xxx/yyy/zzz
# - Discord: https://discord.com/api/webhooks/xxx/yyy
# - Email: smtp://username:password@host:port/?from=sender@example.com&to=recipient@example.com
# - Telegram: telegram://bot_token@telegram?chats=chat_id
# - Gotify: gotify://gotify-host/gotify-token

# ===================================
# GRAFANA CONFIGURATION
# ===================================
# Grafana service settings
GRAFANA_IMAGE=grafana/grafana:latest
GRAFANA_CONTAINER_NAME=aistack-grafana
GRAFANA_PORT=3001

# Grafana admin credentials
# Using simple credentials for development - CHANGE FOR PRODUCTION!
GF_ADMIN_USER=admin
GF_ADMIN_PASSWORD=admin

# Grafana plugins (comma-separated list)
GF_INSTALL_PLUGINS=

# Example plugins:
# GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel

# Grafana server settings
GF_SERVER_ROOT_URL=http://localhost:3001

# Grafana analytics settings
GF_ANALYTICS_ENABLED=false
GF_ANALYTICS_UPDATES=false

# ===================================
# RESOURCE LIMITS & GPU CONFIGURATION
# ===================================
# GPU Access: Configured via deploy.resources.reservations.devices
# This is equivalent to docker run --gpus=all
#
# Requires: docker-compose version 3.9+
# Works with: docker-compose v1.29.2 (when version: "3.9" is specified)
#
# Resource Limits: REMOVED for development flexibility
#   All services can use resources as needed without artificial constraints
#   This allows for maximum performance and flexibility during development
#
# GPU-Enabled Services:
#   - Ollama: deploy.resources.reservations.devices (all GPUs)
#   - ComfyUI: deploy.resources.reservations.devices (all GPUs)
#
# Both services share GPU dynamically:
#   - Ollama will use as much VRAM as available
#   - ComfyUI will use VRAM when generating images
#   - GPU automatically manages memory allocation between services
#
# Note: For production, consider adding resource limits back to prevent
#       resource exhaustion and ensure system stability

# ===================================
# VRAM SHARING STRATEGY (16GB GPU)
# ===================================
# For 5 concurrent users with both chat and media generation:
#
# Allocation Strategy:
# - Ollama: ~10GB max (2 models loaded, 2 parallel requests)
# - ComfyUI: ~6GB dynamically (--normalvram mode)
#
# How It Works:
# 1. Ollama loads models on-demand up to MAX_LOADED_MODELS limit
# 2. When VRAM reaches OLLAMA_MAX_VRAM, oldest unused models unload
# 3. ComfyUI uses --normalvram to dynamically manage remaining VRAM
# 4. Both services release memory when idle
#
# Monitoring Commands:
#   watch -n 1 nvidia-smi                    # Real-time GPU usage
#   docker stats                             # Container resource usage
#   docker logs aistack-ollama | grep VRAM   # Ollama memory logs
#
# Tuning Guidelines:
# - If Ollama OOM: Reduce OLLAMA_MAX_VRAM or MAX_LOADED_MODELS
# - If ComfyUI OOM: Switch to --lowvram mode
# - If both struggle: Reduce OLLAMA_NUM_PARALLEL to 1
# - For larger models: Increase OLLAMA_MAX_VRAM, decrease MAX_LOADED_MODELS
#
# Model Size Reference (approximate VRAM usage):
# - 7B models (Mistral, Llama2): ~4-5GB
# - 13B models: ~7-8GB
# - 30B+ models: ~15GB+ (may not fit with ComfyUI running)
#
# ComfyUI Workflow Size:
# - Simple SDXL: ~3-4GB
# - Flux: ~4-6GB
# - Multiple LoRAs: +1-2GB each

# ===================================
# DEVELOPMENT NOTES
# ===================================
# 1. GPU Configuration
#    - Both Ollama and ComfyUI: Use deploy.resources.reservations.devices
#    - Requires version: "3.9" in docker-compose.yaml
#    - Verify GPU: docker exec aistack-ollama nvidia-smi
#    - Monitor usage: watch -n 1 nvidia-smi
#    - Check VRAM allocation: docker logs aistack-ollama | grep "VRAM"
#    - Check model loading: docker logs aistack-ollama | grep "loaded"
#
# 2. Watchtower is ENABLED in this configuration for automatic updates
#    - Convenient for development
#    - Check logs: docker compose -f docker-compose.dev.yaml logs -f watchtower
#
# 3. Simple passwords are used (admin/admin)
#    - Fine for local development
#    - MUST change for any production or exposed environments
#
# 4. CORS is wide open (OLLAMA_ORIGINS=*)
#    - Allows any origin to access Ollama
#    - Fine for local development
#    - Restrict for production
#
# 5. To switch to production configuration:
#    - Stop: docker compose -f docker-compose.dev.yaml down
#    - Use: docker compose up -d (uses docker-compose.yaml)
#    - Or use .env with production settings

# ===================================
# QUICK START GUIDE
# ===================================
# 1. Start development stack:
#    docker compose -f docker-compose.dev.yaml up -d
#
# 2. Check status:
#    docker compose -f docker-compose.dev.yaml ps
#
# 3. View logs:
#    docker compose -f docker-compose.dev.yaml logs -f
#
# 4. Check Watchtower logs:
#    docker compose -f docker-compose.dev.yaml logs -f watchtower
#
# 5. Force update check:
#    docker exec watchtower /watchtower --run-once
#
# 6. Access services:
#    - Ollama: http://localhost:11434
#    - Open-WebUI: http://localhost:3000
#    - ComfyUI: http://localhost:8188
#    - Grafana: http://localhost:3001 (admin/admin)
