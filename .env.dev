# ===================================
# DOCKER COMPOSE CONFIGURATION - DEVELOPMENT
# ===================================
# Environment variables for docker-compose.dev.yaml
# This is a development configuration with Watchtower enabled
#
# Usage:
#   cp .env.dev .env  # If you want to use dev settings
#   docker compose -f docker-compose.dev.yaml up -d

# ===================================
# GLOBAL SETTINGS
# ===================================
# Restart policy for all services (unless-stopped, always, no, on-failure)
RESTART_POLICY=always

# ===================================
# OLLAMA CONFIGURATION
# ===================================
# Ollama service settings
OLLAMA_IMAGE=ollama/ollama:latest
OLLAMA_CONTAINER_NAME=aistack-ollama
OLLAMA_PORT=11434
OLLAMA_HOST=0.0.0.0

# CORS (Cross-Origin Resource Sharing) allowed origins
# Controls which web domains can access the Ollama API
# "*" = Allow all origins (default, convenient for development)
# For production, specify exact origins for security:
# OLLAMA_ORIGINS=http://localhost:3000,https://yourdomain.com
OLLAMA_ORIGINS=*

# ===================================
# OPEN-WEBUI CONFIGURATION
# ===================================
# Open-WebUI service settings
OPENWEBUI_IMAGE=ghcr.io/open-webui/open-webui:main
OPENWEBUI_CONTAINER_NAME=aistack-open-webui
OPENWEBUI_PORT=3000

# Open-WebUI application settings
OLLAMA_BASE_URL=http://aistack-ollama:11434
WEBUI_SECRET_KEY=dev-secret-key-change-for-production
WEBUI_AUTH=true
WEBUI_NAME="Open WebUI (Dev)"

# ===================================
# COMFYUI CONFIGURATION
# ===================================
# ComfyUI service settings
COMFYUI_IMAGE=saladtechnologies/comfyui:comfy0.3.61-api1.9.2-torch2.8.0-cuda12.8-flux1-schnell-fp8
COMFYUI_CONTAINER_NAME=aistack-comfyui
COMFYUI_PORT=8188
COMFYUI_INTERNAL_PORT=8188

# GPU Memory Management (set in docker-compose.dev.yaml):
# PYTORCH_CUDA_ALLOC_CONF: Controls PyTorch CUDA memory allocation
#   - garbage_collection_threshold:0.6 = More aggressive memory cleanup
#   - max_split_size_mb:128 = Smaller memory chunks for better sharing
# CUDA_MODULE_LOADING=LAZY: Load CUDA modules on-demand (saves memory)

# ===================================
# WATCHTOWER CONFIGURATION (ENABLED FOR DEVELOPMENT)
# ===================================
# üîÑ Watchtower automatically updates containers when new images are available
# ‚ö†Ô∏è  This is enabled for development convenience
# üîí For production, disable this and use ./update-stack.sh instead

# Watchtower service settings
WATCHTOWER_IMAGE=containrrr/watchtower:latest
WATCHTOWER_CONTAINER_NAME=aistack-watchtower

# Watchtower behavior settings
WATCHTOWER_CLEANUP=true
WATCHTOWER_INCLUDE_STOPPED=false

# Update schedule (cron format: minute hour day month weekday)
# Current: Every day at 3 AM
# Examples:
#   0 0 * * * = Every day at midnight
#   0 */6 * * * = Every 6 hours
#   0 0 * * 0 = Every Sunday at midnight
WATCHTOWER_SCHEDULE=0 0 3 * * *

# Enable debug logging (helpful for development)
WATCHTOWER_DEBUG=false

# Watchtower notifications (optional)
# Leave empty to disable notifications
WATCHTOWER_NOTIFICATIONS=
WATCHTOWER_NOTIFICATION_URL=

# Example notification URLs:
# - Slack: https://hooks.slack.com/services/xxx/yyy/zzz
# - Discord: https://discord.com/api/webhooks/xxx/yyy
# - Email: smtp://username:password@host:port/?from=sender@example.com&to=recipient@example.com
# - Telegram: telegram://bot_token@telegram?chats=chat_id
# - Gotify: gotify://gotify-host/gotify-token

# ===================================
# GRAFANA CONFIGURATION
# ===================================
# Grafana service settings
GRAFANA_IMAGE=grafana/grafana:latest
GRAFANA_CONTAINER_NAME=aistack-grafana
GRAFANA_PORT=3001

# Grafana admin credentials
# Using simple credentials for development - CHANGE FOR PRODUCTION!
GF_ADMIN_USER=admin
GF_ADMIN_PASSWORD=admin

# Grafana plugins (comma-separated list)
GF_INSTALL_PLUGINS=

# Example plugins:
# GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource,grafana-piechart-panel

# Grafana server settings
GF_SERVER_ROOT_URL=http://localhost:3001

# Grafana analytics settings
GF_ANALYTICS_ENABLED=false
GF_ANALYTICS_UPDATES=false

# ===================================
# RESOURCE LIMITS & GPU CONFIGURATION
# ===================================
# GPU Access: Configured via 'runtime: nvidia' (docker-compose v1 compatible)
# This is equivalent to docker run --gpus=all
#
# ‚ö†Ô∏è  NOTE: Using docker-compose v1.29.2 (Python-based)
#     The 'runtime: nvidia' syntax is used instead of deploy.resources.reservations
#     which is only supported in Docker Compose v2+
#
# Resource Limits: REMOVED for development flexibility
#   All services can use resources as needed without artificial constraints
#   This allows for maximum performance and flexibility during development
#
# GPU-Enabled Services:
#   - Ollama: runtime: nvidia + NVIDIA env vars
#   - ComfyUI: runtime: nvidia + NVIDIA env vars
#
# NVIDIA Environment Variables (set in compose file):
#   NVIDIA_VISIBLE_DEVICES=all
#   NVIDIA_DRIVER_CAPABILITIES=compute,utility
#
# GPU will automatically detect available VRAM and load as many model layers as possible
#
# Note: For production, consider adding resource limits back to prevent
#       resource exhaustion and ensure system stability

# ===================================
# DEVELOPMENT NOTES
# ===================================
# 1. GPU Configuration
#    - Both Ollama and ComfyUI: Use runtime: nvidia (docker-compose v1 compatible)
#    - Verify GPU: docker exec aistack-ollama nvidia-smi
#    - Monitor usage: watch -n 1 nvidia-smi
#    - Check layers: docker logs aistack-ollama | grep "offloaded"
#    - For Docker Compose v2: Consider upgrading for more GPU control options
#
# 2. Watchtower is ENABLED in this configuration for automatic updates
#    - Convenient for development
#    - Check logs: docker compose -f docker-compose.dev.yaml logs -f watchtower
#
# 3. Simple passwords are used (admin/admin)
#    - Fine for local development
#    - MUST change for any production or exposed environments
#
# 4. CORS is wide open (OLLAMA_ORIGINS=*)
#    - Allows any origin to access Ollama
#    - Fine for local development
#    - Restrict for production
#
# 5. To switch to production configuration:
#    - Stop: docker compose -f docker-compose.dev.yaml down
#    - Use: docker compose up -d (uses docker-compose.yaml)
#    - Or use .env with production settings

# ===================================
# QUICK START GUIDE
# ===================================
# 1. Start development stack:
#    docker compose -f docker-compose.dev.yaml up -d
#
# 2. Check status:
#    docker compose -f docker-compose.dev.yaml ps
#
# 3. View logs:
#    docker compose -f docker-compose.dev.yaml logs -f
#
# 4. Check Watchtower logs:
#    docker compose -f docker-compose.dev.yaml logs -f watchtower
#
# 5. Force update check:
#    docker exec watchtower /watchtower --run-once
#
# 6. Access services:
#    - Ollama: http://localhost:11434
#    - Open-WebUI: http://localhost:3000
#    - ComfyUI: http://localhost:8188
#    - Grafana: http://localhost:3001 (admin/admin)
